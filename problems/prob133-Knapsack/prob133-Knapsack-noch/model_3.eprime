language ESSENCE' 1.0

given U_EnumSize: int
given p1: int
given s_Function1D: matrix indexed by [int(1..U_EnumSize)] of int(1..)
given p2: int
given v_Function1D: matrix indexed by [int(1..U_EnumSize)] of int(1..)
given B: int(1..)
given K: int(1..)
find U'_ExplicitVarSizeWithFlags_Flags:
        matrix indexed by [int(1..1 + (U_EnumSize - 1))] of bool
find U'_ExplicitVarSizeWithFlags_Values:
        matrix indexed by [int(1..1 + (U_EnumSize - 1))] of int(1..U_EnumSize)
such that
    sum([U'_ExplicitVarSizeWithFlags_Flags[q2] *
         s_Function1D[U'_ExplicitVarSizeWithFlags_Values[q2]]
             | q2 : int(1..1 + (U_EnumSize - 1))])
    <= B,
    sum([U'_ExplicitVarSizeWithFlags_Flags[q3] *
         v_Function1D[U'_ExplicitVarSizeWithFlags_Values[q3]]
             | q3 : int(1..1 + (U_EnumSize - 1))])
    >= K,
    and([U'_ExplicitVarSizeWithFlags_Flags[q1 + 1] ->
         U'_ExplicitVarSizeWithFlags_Values[q1] <
         U'_ExplicitVarSizeWithFlags_Values[q1 + 1]
             | q1 : int(1..1 + (U_EnumSize - 1) - 1)]),
    and([U'_ExplicitVarSizeWithFlags_Flags[q1] = false ->
         U'_ExplicitVarSizeWithFlags_Values[q1] = 1
             | q1 : int(1..1 + (U_EnumSize - 1))]),
    and([U'_ExplicitVarSizeWithFlags_Flags[q1 + 1] ->
         U'_ExplicitVarSizeWithFlags_Flags[q1]
             | q1 : int(1..1 + (U_EnumSize - 1) - 1)])

